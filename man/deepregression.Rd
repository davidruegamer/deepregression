% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/deepregression.R
\name{deepregression}
\alias{deepregression}
\title{Fitting Semi-Structured Deep Distributional Regression}
\usage{
deepregression(
  y,
  list_of_formulas,
  list_of_deep_models,
  family = "normal",
  train_together = list(),
  data,
  tf_seed = as.integer(1991 - 5 - 4),
  return_prepoc = FALSE,
  subnetwork_builder = subnetwork_init,
  model_builder = keras_dr,
  smooth_options = smooth_control(),
  orthog_options = orthog_control(),
  ...
)
}
\arguments{
\item{y}{response variable}

\item{list_of_formulas}{a named list of right hand side formulas,
one for each parameter of the distribution specified in \code{family};
set to \code{~ 1} if the parameter should be treated as constant.
Use the \code{s()}-notation from \code{mgcv} for specification of
non-linear structured effects and \code{d(...)} for
deep learning predictors (predictors in brackets are separated by commas),
where \code{d} can be replaced by an name name of the names in
\code{list_of_deep_models}, e.g., \code{~ 1 + s(x) + my_deep_mod(a,b,c)},
where my_deep_mod is the name of the neural net specified in
\code{list_of_deep_models} and \code{a,b,c} are features modeled via
this network.}

\item{list_of_deep_models}{a named list of functions
specifying a keras model.
See the examples for more details.}

\item{family}{a character specifying the distribution. For information on
possible distribution and parameters, see \code{\link{make_tfd_dist}}. Can also 
be a custom distribution}

\item{train_together}{a list of formulas of the same length as \code{list_of_formulas}
specifying the deep predictors that should be trained together and then the results are;
fed into different distribution parameters; use the same name for the deep predictor to
indicate for which distribution parameter they should be used. For example, if the second
and fourth list entry are \code{~ lstm_mod(text)} then the jointly learned \code{lstm_mod}
network is added to the linear predictor of the second and fourth distribution parameter.
Those network names should then be excluded from the \code{list_of_formulas}}

\item{data}{data.frame or named list with input features}

\item{tf_seed}{a seed for tensorflow (only works with R version >= 2.2.0)}

\item{return_prepoc}{logical; if TRUE only the pre-processed data and layers are returned (default FALSE).}

\item{subnetwork_builder}{function to build each subnetwork (network for each distribution parameter;
per default \code{subnetwork_init})}

\item{model_builder}{function to build the model based on additive predictors (per default \code{keras_dr}).
In order to work with the methods defined for the class \code{deepregression}, the model should behave
like a keras model}

\item{...}{further arguments passed to the \code{model_builder} function}
}
\description{
Fitting Semi-Structured Deep Distributional Regression
}
\examples{
library(deepregression)

n <- 1000
data = data.frame(matrix(rnorm(4*n), c(n,4)))
colnames(data) <- c("x1","x2","x3","xa")
formula <- ~ 1 + deep_model(x1,x2,x3) + s(xa) + x1

deep_model <- function(x) x \%>\%
layer_dense(units = 32, activation = "relu", use_bias = FALSE) \%>\%
layer_dropout(rate = 0.2) \%>\%
layer_dense(units = 8, activation = "relu") \%>\%
layer_dense(units = 1, activation = "linear")

y <- rnorm(n) + data$xa^2 + data$x1

mod <- deepregression(
  list_of_formulas = list(loc = formula, scale = ~ 1),
  data = data, y = y,
  list_of_deep_models = list(deep_model = deep_model)
)

# train for more than 10 epochs to get a better model
mod \%>\% train(epochs = 10, early_stopping = TRUE)
mod \%>\% fitted()
mod \%>\% plot()
mod \%>\% coef()

}
